{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intellectual-romance",
   "metadata": {},
   "source": [
    "# Identification of a Neural-Network for Constitutive Law"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-excitement",
   "metadata": {},
   "source": [
    "## Preliminary part - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-nomination",
   "metadata": {},
   "source": [
    "To allow inline pictures, run the following block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-jimmy",
   "metadata": {},
   "source": [
    "Import all the useful libraries before first run\n",
    "We need here the classic ones such as:\n",
    "- math\n",
    "- numpy\n",
    "- pandas\n",
    "- matplotlib\n",
    "\n",
    "And for the Neural Network, we also need to import parts of the keras module of TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importation de TensorFlow\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.metrics import MeanSquaredError\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-bride",
   "metadata": {},
   "source": [
    "**Global definitions**\n",
    "\n",
    "Here after, we define the colors to use for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-helmet",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFigures = True\n",
    "colorlist = ['#bb0000', '#00bb00', \"#0000bb\", '#bbbb00', '#bb00bb', \"#00bbbb\", '#bbbbbb', '#770000', '#007700', \"#000077\", '#777700', '#770077', \"#007777\", '#777777', '#440000', '#004400', \"#000044\", '#444400', '#440044', \"#0044444\", '#444444','#000000']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-fishing",
   "metadata": {},
   "source": [
    "Useful functions to define the number of subplots to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseSize = (8, 6)  # Base size of a subplot\n",
    "\n",
    "def sbPlot(n):\n",
    "    if (n == 1): return 1, 1\n",
    "    if (n <= 2): return 1, 2\n",
    "    if (n <= 4): return 2, 2\n",
    "    if (n <= 6): return 3, 2\n",
    "    if (n <= 9): return 3, 3\n",
    "    if (n <= 12): return 4, 3\n",
    "    return 0, 0\n",
    "\n",
    "def sbPlotSize(n):\n",
    "    x, y = sbPlot(n)\n",
    "    return baseSize[0] * y, baseSize[1] * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-jefferson",
   "metadata": {},
   "source": [
    "Used to plot an Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDigitsFrom(n):\n",
    "    d = int(math.log10(abs(n)))\n",
    "    return n*10**(-d + 1), d - 1\n",
    "\n",
    "def plotHisto(Y, labels, bottom, top, xlab, ylab, figname, title):\n",
    "    numb = len(Y)\n",
    "    X = np.arange(numb) +1\n",
    "\n",
    "    plt.figure(figsize = (12, 9)) # for a4 landscape\n",
    "    plt.rc('text', usetex=True)\n",
    "    plt.rcParams['xtick.labelsize'] = 16\n",
    "    plt.rcParams['ytick.labelsize'] = 16\n",
    "\n",
    "    plt.bar(X, Y, color = '#770000')\n",
    "\n",
    "    for x, y in zip(X, Y):\n",
    "        u,v = getDigitsFrom(y)\n",
    "        plt.text(x, y + 0.01*(top-bottom), '$%.2f \\\\times 10^{%d}$' % (u,v), ha='center', va='bottom', fontsize=16)\n",
    "\n",
    "    for i in range(0,numb):\n",
    "        if (Y[i]> 0.3*top):\n",
    "            plt.text(X[i], (top-bottom)/10 + bottom , labels[i], ha='center', va='bottom', fontsize=20, rotation=90, color='white')\n",
    "        else:\n",
    "            plt.text(X[i], 5*(top-bottom)/10 + bottom , labels[i], ha='center', va='bottom', fontsize=20, rotation=90, color='black')\n",
    "\n",
    "    plt.ylim(bottom, top)\n",
    "\n",
    "    plt.ylabel(ylab, fontsize = 20)\n",
    "    plt.xlabel(xlab, fontsize = 20)\n",
    "    plt.title(title, fontsize = 20)\n",
    "\n",
    "    plt.tick_params(axis='x',top=False, bottom=False, left=False, right=False,\n",
    "                labelleft=False, labelbottom=False)\n",
    "    plt.grid(True)\n",
    "    if (saveFigures) : plt.savefig(figname)\n",
    "    else: plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-supervision",
   "metadata": {},
   "source": [
    "## Definition of the data file to use \n",
    "We define here after the directory and the Excel Datafile used for the Neural-Network construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-anthony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataPath = 'ANN-Zhou-Law'\n",
    "# dataFile = 'ExperimentsTreated.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = 'ANN-JohnsonCook'\n",
    "dataFile = 'JC-Experiments.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-closure",
   "metadata": {},
   "source": [
    "## Open the Excel file and import data\n",
    "Read the content of the Excel file using the pandas library method read_excel.\n",
    "\n",
    "In this datafile, temperatures must be on separate sheets, while the corresponding sheets contains stress / strain / strain-rate values: $$\\overline{\\sigma}=f({\\overline{\\varepsilon}^p},\\dot{\\overline{\\varepsilon}^p})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "excelData = pd.read_excel(dataPath + '/' + dataFile, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-accessory",
   "metadata": {},
   "source": [
    "Extract the list of temperatures $T$ from the the Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(excelData)\n",
    "temperatures = []\n",
    "temps = list(excelData.keys())\n",
    "for T in temps:\n",
    "    temperatures.append(int(T.replace('째C', '')))\n",
    "temperaturesOrig = temperatures\n",
    "nTemp = len(temperatures)\n",
    "nTemp, temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-tunnel",
   "metadata": {},
   "source": [
    "Extract the list of plastic strain rates $\\dot{\\overline{\\varepsilon}^p}$ from the the Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSheet = excelData[str(temperatures[0]) + '째C']\n",
    "epsp = []\n",
    "for i in range(1, dataSheet.shape[1]):\n",
    "    epsp.append(dataSheet.columns[i])\n",
    "epspOrig = epsp\n",
    "nEpsp = len(epsp)\n",
    "nEpsp, epsp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-child",
   "metadata": {},
   "source": [
    "Extract all data from the panda sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read first sheet\n",
    "dataSheet = excelData[str(temperatures[0]) + '째C']\n",
    "data = dataSheet.values\n",
    "eps = data[:, 0]\n",
    "sig = data[:, 1:]\n",
    "# Append the other sheets\n",
    "for T in temperatures[1:]:\n",
    "    dataSheet = excelData[str(T) + '째C']\n",
    "    data = dataSheet.values\n",
    "    sig = np.append(sig, data[:, 1:], axis=1)\n",
    "nEps = len(eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-production",
   "metadata": {},
   "source": [
    "Creates two arrays for temperatures $T$ and plastic strain rates $\\dot{\\overline{\\varepsilon}^p}$ and transforms the values of the plastic strain rates $\\dot{\\overline{\\varepsilon}^p}$ into its logarithmic part.\n",
    "\n",
    "**It is very important** to transform the epsp data into its logarithmic part since entries of the ANN show enhance a linear behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = np.array(temperatures)\n",
    "epsp = np.array(epsp)\n",
    "epsLogBase = epsp.min()\n",
    "epsp = np.log(epsp / epsLogBase)\n",
    "epspArray = np.tile(epsp, temperatures.shape[0])\n",
    "temperaturesArray = temperatures.repeat(epsp.shape[0])\n",
    "epspArray.shape, epspArray, temperaturesArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-danish",
   "metadata": {},
   "source": [
    "Transform all data into an array of 4 columns where:\n",
    "- $\\overline{\\varepsilon}^p$ : is in first column\n",
    "- $\\dot{\\overline{\\varepsilon}^p}$ : is in second column\n",
    "- $T$ : is in third column\n",
    "- $\\overline{\\sigma}$ : is th fourth column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-lightning",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entries = np.empty([epspArray.shape[0] * eps.shape[0], 4])\n",
    "row = 0\n",
    "colEps = 0\n",
    "colEpsp = 1\n",
    "colT = 2\n",
    "colSig = 3\n",
    "for i in range(epspArray.shape[0]):\n",
    "    for j in range(eps.shape[0]):\n",
    "        entries[row, colEps] = eps[j]\n",
    "        entries[row, colEpsp] = epspArray[i]\n",
    "        entries[row, colT] = temperaturesArray[i]\n",
    "        entries[row, colSig] = sig[j, i]\n",
    "        row += 1\n",
    "entries.shape, entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-badge",
   "metadata": {},
   "source": [
    "Then, we must normalize all the entries, so that they vary within the range $[0:1]$.\n",
    "\n",
    "- We compute the minEntries and rangeEntries variable that contains all the min and max Values\n",
    "- We normalize the data by dividing all variables using: $$X_n = \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n",
    "\n",
    "To rebuid data, we must use: $$ X = X_n(X_{max} - X_{min}) + X_{min}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "minEntries = entries.min(axis=0)\n",
    "maxEntries = entries.max(axis=0)\n",
    "rangeEntries = maxEntries - minEntries\n",
    "NNentries = (entries - minEntries) / rangeEntries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-ending",
   "metadata": {},
   "source": [
    "To check the previous process precision, and lost of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-programmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max error for normalized data is :\",(NNentries * rangeEntries + minEntries - entries).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-still",
   "metadata": {},
   "source": [
    "## Plot the original data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-winner",
   "metadata": {},
   "source": [
    "Plots the original data where subplots are dependents on the temperature $T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-deployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = sbPlotSize(nTemp))\n",
    "plt.rc('text', usetex = True)\n",
    "plt.subplots_adjust(hspace = 0.3)\n",
    "idx = 1\n",
    "for T in temperatures:\n",
    "    xs, ys = sbPlot(nTemp)\n",
    "    plt.subplot(xs, ys, idx)\n",
    "    cl = 0\n",
    "    for epspv, epspvv in zip(epsp, epspOrig):\n",
    "        # filter on T\n",
    "        data = NNentries[(entries[:,colT]==T) & (entries[:,colEpsp]==epspv), :]\n",
    "        # Plot the curves\n",
    "        plt.plot(data[:,colEps]*rangeEntries[colEps]+minEntries[colEps], data[:,colSig]*rangeEntries[colSig]+minEntries[colEps], color=colorlist[cl], label=r'$\\dot{\\overline{\\varepsilon}^p}=' + str(epspvv) + '\\ s^{-1}$', marker='o', linewidth = 3)\n",
    "        cl += 1\n",
    "    plt.legend(loc = 'lower right',fancybox = True, numpoints = 1, fontsize = 10)\n",
    "    plt.grid()\n",
    "    plt.xlabel(r'$Deformation\\ \\overline{\\varepsilon}^{p}$', fontsize = 16)\n",
    "    plt.ylabel(r'$Stress\\ \\overline{\\sigma}$', fontsize = 16)\n",
    "    plt.title(r'$T=' + str(T) + '^{\\circ}C$', fontsize = 16)\n",
    "    idx += 1\n",
    "if (saveFigures) : plt.savefig(dataPath + '/' + 'OriginalData-T.svg', bbox_inches = 'tight', pad_inches = 0)\n",
    "else : plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-celebration",
   "metadata": {},
   "source": [
    "Plots the original data where subplots are dependents on the plastic strain rate $\\dot{\\overline{\\varepsilon}^p}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = sbPlotSize(nEpsp))\n",
    "plt.rc('text', usetex = True)\n",
    "plt.subplots_adjust(hspace = 0.3)\n",
    "idx = 1\n",
    "for epspv, epspvv in zip(epsp, epspOrig):\n",
    "    xs, ys = sbPlot(nEpsp)\n",
    "    plt.subplot(xs, ys, idx)\n",
    "    cl = 0\n",
    "    for T, TOrig in zip(temperatures, temperaturesOrig):\n",
    "        # filter on T\n",
    "        data = NNentries[(entries[:,colT]==T) & (entries[:,colEpsp]==epspv), :]\n",
    "        # Plot the curves\n",
    "        plt.plot(data[:,colEps]*rangeEntries[colEps]+minEntries[colEps], data[:,colSig]*rangeEntries[colSig]+minEntries[colSig], color=colorlist[cl], label=r'$T=' + str(TOrig) + '^{\\circ}C$', marker = 'o', linewidth = 3)\n",
    "        cl += 1\n",
    "    plt.legend(loc = 'lower right',fancybox = True, numpoints = 1, fontsize = 10)\n",
    "    plt.grid()\n",
    "    plt.xlabel(r'$Deformation\\ \\overline{\\varepsilon}^{p}$', fontsize = 16)\n",
    "    plt.ylabel(r'$Stress\\ \\overline{\\sigma}$', fontsize = 16)\n",
    "    plt.title(r'$\\dot{\\overline{\\varepsilon}^p}=' + str(epspvv) + '\\ s^{-1}$', fontsize = 16)\n",
    "    idx += 1\n",
    "if (saveFigures) : plt.savefig(dataPath + '/' + 'OriginalData-epsp.svg', bbox_inches = 'tight', pad_inches = 0)\n",
    "else: plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-council",
   "metadata": {},
   "source": [
    "## Data form transformation for the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-thesis",
   "metadata": {},
   "source": [
    "Separate input and output from data.\n",
    "\n",
    "Input contains:\n",
    "- $\\overline{\\varepsilon}^p$ : is in first column\n",
    "- $\\dot{\\overline{\\varepsilon}^p}$ : is in second column\n",
    "- $T$ : is in third column\n",
    "\n",
    "Output contains:\n",
    "- $\\overline{\\sigma}$ : the only column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "NNinput = NNentries[:,colEps:colSig]\n",
    "NNoutput = NNentries[:,colSig]\n",
    "NNinput.shape, NNoutput.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-poison",
   "metadata": {},
   "source": [
    "## Neural Network set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []  # The list of models to use\n",
    "convergenceDatas = []   # To store convergence curves\n",
    "\n",
    "# FACT = ['tanh', 'sigmoid']\n",
    "# COUCH = [5, 7, 9, [5,3], [7,4], [9,5]]\n",
    "\n",
    "FACT = ['sigmoid']\n",
    "COUCH = [[15, 7]]\n",
    "\n",
    "for f in FACT:\n",
    "    for c in COUCH:\n",
    "        desc = '3'\n",
    "        model = Sequential()\n",
    "        if type(c) == list:\n",
    "            fst = True\n",
    "            for k in c:\n",
    "                if (fst): model.add(Dense(k, input_dim = 3, activation = f))\n",
    "                else: model.add(Dense(k, activation = f))\n",
    "                fst = False\n",
    "                desc += '-' + str(k)\n",
    "        else:\n",
    "            model.add(Dense(c, input_dim = 3, activation = f))\n",
    "            desc += '-' + str(c)\n",
    "        model.add(Dense(1))\n",
    "        desc += '-1-' + f\n",
    "        model._name = desc\n",
    "        models.append(model)\n",
    "        convergenceDatas.append([desc, np.array([])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #opt0 = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "# opt0 = optimizers.Adam()\n",
    "# opt1 = optimizers.Adam()\n",
    "# opt2 = optimizers.Adam()\n",
    "# opt3 = optimizers.Adam(amsgrad=True)\n",
    "\n",
    "# models[0].compile(loss = 'mean_squared_error', optimizer = opt0)\n",
    "# models[1].compile(loss = 'mean_squared_error', optimizer = opt1, metrics = ['accuracy'])\n",
    "# models[2].compile(loss = 'mean_squared_error', optimizer = opt2, metrics = ['mean_squared_error'])\n",
    "# models[3].compile(loss = 'mean_squared_error', optimizer = opt3)\n",
    "# models[0]._name = models[0].name + \"-none\"\n",
    "# models[1]._name = models[1].name + \"-accuracy\"\n",
    "# models[2]._name = models[2].name + \"-mse\"\n",
    "# models[3]._name = models[3].name + \"-amsgrad\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-crystal",
   "metadata": {},
   "source": [
    "Initialize models and apply solver and optimizer for each one.\n",
    "\n",
    "Here, we choose the **adam** solver and a **mse** error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code, I don't even remember\n",
    "# # Load the test Database\n",
    "# testFileName = dataPath + '/Datatest.npz'\n",
    "# testDatabase = np.load(testFileName)\n",
    "# testData = testDatabase['testData']\n",
    "\n",
    "# # Number of rows of the test database\n",
    "# nrows = testData.shape[0]\n",
    "\n",
    "# # Extract data from the database\n",
    "# eps_test = testData[:,0].reshape(nrows,1)\n",
    "# epsp_test = testData[:,1].reshape(nrows,1)\n",
    "# T_test = testData[:,2].reshape(nrows,1)\n",
    "# sig_test = testData[:,3].reshape(nrows,1)\n",
    "\n",
    "# # Computes the log of epsp\n",
    "# epsp_test = np.log(epsp_test/epsLogBase)\n",
    "\n",
    "# # Creates the input test data with scale\n",
    "# inputTest = (np.hstack([eps_test, epsp_test, T_test]) - np.array(minEntries)[0:3]) / np.array(rangeEntries)[0:3]\n",
    "# sig_test = (sig_test- np.array(minEntries)[3]) / np.array(rangeEntries)[3]\n",
    "# inputTest, sig_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-correction",
   "metadata": {},
   "source": [
    "Save all internal arrays for direct use of the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveInternalMatrices(ANN, filename):\n",
    "    if len(ANN.layers) == 1: \n",
    "        np.savez(filename,\n",
    "                 logBase = np.array([epsLogBase]),\n",
    "                 minEntries = minEntries, \n",
    "                 maxEntries = maxEntries,\n",
    "                 w1 = ANN.layers[0].get_weights()[0].T, \n",
    "                 b1 = ANN.layers[0].get_weights()[1].reshape(len(ANN.layers[0].get_weights()[1]),1))\n",
    "    if len(ANN.layers) == 2: \n",
    "        np.savez(filename, \n",
    "                 logBase = np.array([epsLogBase]),\n",
    "                 minEntries = minEntries, \n",
    "                 maxEntries = maxEntries,\n",
    "                 w1 = ANN.layers[0].get_weights()[0].T, \n",
    "                 b1 = ANN.layers[0].get_weights()[1].reshape(len(ANN.layers[0].get_weights()[1]),1),\n",
    "                 w2 = ANN.layers[1].get_weights()[0].T, \n",
    "                 b2 = ANN.layers[1].get_weights()[1].reshape(len(ANN.layers[1].get_weights()[1]),1))\n",
    "    if len(ANN.layers) == 3: \n",
    "        np.savez(filename, \n",
    "                 logBase = np.array([epsLogBase]),\n",
    "                 minEntries = minEntries, \n",
    "                 maxEntries = maxEntries,\n",
    "                 w1 = ANN.layers[0].get_weights()[0].T, \n",
    "                 b1 = ANN.layers[0].get_weights()[1].reshape(len(ANN.layers[0].get_weights()[1]),1),\n",
    "                 w2 = ANN.layers[1].get_weights()[0].T, \n",
    "                 b2 = ANN.layers[1].get_weights()[1].reshape(len(ANN.layers[1].get_weights()[1]),1),\n",
    "                 w3 = ANN.layers[2].get_weights()[0].T, \n",
    "                 b3 = ANN.layers[2].get_weights()[1].reshape(len(ANN.layers[2].get_weights()[1]),1)) \n",
    "    if len(ANN.layers) == 4: \n",
    "        np.savez(filename, \n",
    "                 logBase = np.array([epsLogBase]),\n",
    "                 minEntries = minEntries, \n",
    "                 maxEntries = maxEntries,\n",
    "                 w1 = ANN.layers[0].get_weights()[0].T, \n",
    "                 b1 = ANN.layers[0].get_weights()[1].reshape(len(ANN.layers[0].get_weights()[1]),1),\n",
    "                 w2 = ANN.layers[1].get_weights()[0].T, \n",
    "                 b2 = ANN.layers[1].get_weights()[1].reshape(len(ANN.layers[1].get_weights()[1]),1),\n",
    "                 w3 = ANN.layers[2].get_weights()[0].T, \n",
    "                 b3 = ANN.layers[2].get_weights()[1].reshape(len(ANN.layers[2].get_weights()[1]),1),\n",
    "                 w4 = ANN.layers[3].get_weights()[0].T, \n",
    "                 b4 = ANN.layers[3].get_weights()[1].reshape(len(ANN.layers[3].get_weights()[1]),1))             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-adaptation",
   "metadata": {},
   "source": [
    "## Solve the parameters of all ANN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-struggle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iterationsNumber = 50 # Define the number of iteration to do\n",
    "epochNumber = 1000    # Define the number of epoch for each iteration\n",
    "subSave = True\n",
    "\n",
    "for model, convergenceData in zip (models, convergenceDatas):\n",
    "    print(\"MODEL :\", model.name)\n",
    "    if (subSave):\n",
    "        try:\n",
    "            os.mkdir(dataPath + '/' + model.name)\n",
    "        except:\n",
    "            pass\n",
    "    for i in range(iterationsNumber):\n",
    "        history = model.fit(NNinput, NNoutput, epochs = epochNumber, verbose = 0, shuffle = True)\n",
    "        loss = history.history['loss']\n",
    "        convergenceData[1] = np.append(convergenceData[1], loss)\n",
    "        if (subSave):\n",
    "            saveInternalMatrices(model, dataPath + '/' + model.name + '/ANN-' + str(i))\n",
    "        print(\"Iteration :\", i + 1, \"/\", iterationsNumber, '-> %8.6E' % loss[-1], \"  \", end = '\\r')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-organizer",
   "metadata": {},
   "source": [
    "Save the Convergence curve database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "for convergenceData in convergenceDatas:\n",
    "    name = convergenceData[0]\n",
    "    convCurve = convergenceData[1]\n",
    "    np.savez(dataPath + '/CD-' + name, convCurve = convCurve)\n",
    "    print(\"Convergence data %s saved\" % (name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-laser",
   "metadata": {},
   "source": [
    "Load the Convergence database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = ['3-5-1-tanh', '3-7-1-tanh']\n",
    "# convergenceDatas = []   # To store convergence curves\n",
    "# for name in names:\n",
    "#     NN = np.load(dataPath + '/CD-' + name + '.npz')\n",
    "#     cc = [name, NN['convCurve']]\n",
    "#     convergenceDatas.append(cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-dryer",
   "metadata": {},
   "source": [
    "## Plot results of the optimization procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-cloud",
   "metadata": {},
   "source": [
    "First, we plot the convergence curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-retro",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 9))\n",
    "plt.rc('text', usetex = True)\n",
    "cl = 0\n",
    "for convergenceData in convergenceDatas:\n",
    "    name = convergenceData[0]\n",
    "    curve = convergenceData[1]\n",
    "    shortCurve = curve[::10]\n",
    "    x = np.linspace(0, len(curve), len(shortCurve))\n",
    "    plt.plot(x, shortCurve, label = name, color=colorlist[cl], linewidth = 3)\n",
    "    cl += 1\n",
    "plt.grid()\n",
    "plt.xlabel(r'$Iteration$', fontsize = 16)\n",
    "plt.ylim(0, 5e-3)\n",
    "plt.ylabel(r'$Quadratic\\ Error\\ \\Delta E$', fontsize = 16)\n",
    "plt.title(r'$Global\\ convergence\\ of\\ the\\ Neural\\ Network$', fontsize = 16)\n",
    "plt.legend(loc = 'upper right',fancybox = True, numpoints = 1, fontsize = 14)\n",
    "if (saveFigures) : plt.savefig(dataPath + '/' + 'Convergence.svg', bbox_inches = 'tight', pad_inches = 0)\n",
    "else: plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-suspect",
   "metadata": {},
   "source": [
    "The we plot the history of all models precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-necklace",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 9))\n",
    "plt.rc('text', usetex = True)\n",
    "convs = np.array([])\n",
    "labels = []\n",
    "for convergenceData in convergenceDatas:\n",
    "    name = convergenceData[0]\n",
    "    curve = convergenceData[1]\n",
    "    lastzone = int(len(curve)/20) # 5% des points pris en compte pour la convergence globale\n",
    "    convs = np.append(convs, curve[-lastzone:].mean())\n",
    "    labels.append(name)\n",
    "bottom = 0\n",
    "top = convs.max()*1.1\n",
    "ylab = r'$Quadratic\\ Error\\ \\Delta E$'\n",
    "xlab = r'$Model\\ used$'\n",
    "figname = dataPath + '/' + 'Precision.svg'\n",
    "title = r'$Global\\ precision\\ of\\ the\\ Neural\\ Network$'\n",
    "plotHisto(convs, labels, bottom, top, xlab, ylab, figname, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-highlight",
   "metadata": {},
   "source": [
    "Plots the results of the model where all subplots are dependents on the plastic strain rate $\\dot{\\overline{\\varepsilon}^p}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-racing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    predictedOutput = model.predict(NNinput)\n",
    "    plotPredicted = predictedOutput.reshape(nEpsp*nTemp, nEps).T\n",
    "    pcol = 0\n",
    "    plt.figure(figsize = sbPlotSize(nTemp))\n",
    "    plt.rc('text', usetex = True)\n",
    "    plt.subplots_adjust(hspace = 0.3)\n",
    "    idx = 1\n",
    "    for T, TOrig in zip(temperatures, temperaturesOrig):\n",
    "        pli = 0\n",
    "        xs, ys = sbPlot(nTemp)\n",
    "        plt.subplot(xs, ys, idx)\n",
    "        for epspv, epspvAff in zip(epsp, epspOrig):\n",
    "            # filter on T\n",
    "            data = NNentries[(entries[:,colT]==T) & (entries[:,colEpsp]==epspv), :]\n",
    "            # Plot the curves\n",
    "            plt.plot(data[:,colEps]*rangeEntries[colEps]+minEntries[colEps], data[:,colSig]*rangeEntries[colSig]+minEntries[colSig], color=colorlist[pli], label=r'$\\dot{\\overline{\\varepsilon}^p}=' + str(epspvAff) + '\\ s^{-1}$', marker = 'o', linestyle='none')\n",
    "            plt.plot(data[:,colEps]*rangeEntries[colEps]+minEntries[colEps], plotPredicted[:,pcol*nEpsp+pli]*rangeEntries[colSig]+minEntries[colSig], colorlist[pli], linewidth = 3)\n",
    "            pli += 1\n",
    "        pcol += 1\n",
    "        plt.legend(loc = 'lower right',fancybox = True, numpoints = 1, fontsize = 14)\n",
    "        plt.grid()\n",
    "        plt.xlabel(r'$Deformation\\ \\overline{\\varepsilon}^{p}$', fontsize = 16)\n",
    "        plt.ylabel(r'$Stress\\ \\overline{\\sigma}$', fontsize = 16)\n",
    "        plt.title(r'$T=' + str(TOrig) + '^{\\circ}C$', fontsize = 16)\n",
    "        idx += 1\n",
    "    if (saveFigures) : plt.savefig(dataPath + '/' + 'NN-' + model.name + '-epsp.svg', bbox_inches = 'tight', pad_inches = 0)\n",
    "    else: plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-interest",
   "metadata": {},
   "source": [
    "Plots the results of the model where all subplots are dependents on the temperature $T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    predictedOutput = model.predict(NNinput)\n",
    "    plotPredicted = predictedOutput.reshape(nEpsp*nTemp, nEps).T\n",
    "    pcol = 0\n",
    "    plt.figure(figsize = sbPlotSize(nEpsp))\n",
    "    plt.rc('text', usetex = True)\n",
    "    plt.subplots_adjust(hspace = 0.3)\n",
    "    idx = 1\n",
    "    for epspv, epspvAff in zip(epsp, epspOrig):\n",
    "        pli = 0\n",
    "        xs, ys = sbPlot(nEpsp)\n",
    "        plt.subplot(xs, ys, idx)\n",
    "        for T, TOrig in zip(temperatures, temperaturesOrig):\n",
    "            # filter on T\n",
    "            data = NNentries[(entries[:,colT]==T) & (entries[:,colEpsp]==epspv), :]\n",
    "            # Plot the curves\n",
    "            plt.plot(data[:,colEps]*rangeEntries[colEps]+minEntries[colEps], data[:,colSig]*rangeEntries[colSig]+minEntries[colSig], color=colorlist[pli], label=r'$T=' + str(TOrig) + '^{\\circ}C$', marker = 'o', linestyle='none')\n",
    "            plt.plot(data[:,colEps]*rangeEntries[colEps]+minEntries[colEps], plotPredicted[:,pcol+nEpsp*pli]*rangeEntries[colSig]+minEntries[colSig], colorlist[pli], linewidth = 3)\n",
    "            pli += 1\n",
    "        pcol += 1\n",
    "        plt.legend(loc = 'lower right',fancybox = True, numpoints = 1, fontsize = 14)\n",
    "        plt.grid()\n",
    "        plt.xlabel(r'$Deformation\\ \\overline{\\varepsilon}^{p}$', fontsize = 16)\n",
    "        plt.ylabel(r'$Stress\\ \\overline{\\sigma}$', fontsize = 16)\n",
    "        plt.title(r'$\\dot{\\overline{\\varepsilon}^p}=' + str(epspvAff) + '\\ s^{-1}$', fontsize = 16)\n",
    "        idx += 1\n",
    "    if (saveFigures) : plt.savefig(dataPath + '/' + 'NN-' + model.name + '-T.svg', bbox_inches = 'tight', pad_inches = 0)\n",
    "    else: plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-inside",
   "metadata": {},
   "source": [
    "## Save results for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-arabic",
   "metadata": {},
   "source": [
    "Save the Tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-garden",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    model.save(dataPath + '/' + model.name + '/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-intention",
   "metadata": {},
   "source": [
    "Load the Tensorflow models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = []              # The list of models to use\n",
    "# modNames = ['3-15-7-1-sigmoid']\n",
    "# for modName in modNames:\n",
    "#     new_model = tf.keras.models.load_model(dataPath + '/' + modName + '/model')\n",
    "#     models.append(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-needle",
   "metadata": {},
   "source": [
    "Saves the internal matrices data for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-folder",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    saveInternalMatrices(model, dataPath + '/' + model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-parish",
   "metadata": {},
   "source": [
    "## Some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-processor",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    predictedOutput = model.predict(NNinput).T\n",
    "    #np.mean((NNoutput-predictedOutput)**2)\n",
    "    num = np.sum((NNoutput-np.mean(NNoutput))*(predictedOutput-np.mean(predictedOutput)))\n",
    "    den = math.sqrt(np.sum((NNoutput-np.mean(NNoutput))**2)*np.sum((predictedOutput-np.mean(predictedOutput))**2))\n",
    "    R = num/den\n",
    "    #np.mean(np.abs(((NNoutput+1)-(predictedOutput+1))/(NNoutput+1)))*100\n",
    "    plt.figure(figsize = (12, 8))\n",
    "    plt.rc('text', usetex = True)\n",
    "    plt.scatter(NNoutput* rangeEntries[3] + minEntries[3], predictedOutput* rangeEntries[3] + minEntries[3], color='red', label=\"$AN\\!N\\ prediction$\")\n",
    "    plt.plot([minEntries[3],rangeEntries[3] + minEntries[3]], [minEntries[3],rangeEntries[3] + minEntries[3]], color = 'black', label='$Best\\ fit$')\n",
    "    plt.xlim(minEntries[3], rangeEntries[3] + minEntries[3])\n",
    "    plt.ylim(minEntries[3],rangeEntries[3] + minEntries[3])\n",
    "    plt.grid()\n",
    "    plt.legend(loc = 'lower right',fancybox = True, numpoints = 1, fontsize = 14)\n",
    "    plt.xlabel(r'$Analytical\\ flow\\ stress\\ \\overline{\\sigma}$', fontsize = 16)\n",
    "    plt.ylabel(r'$Predicted\\ flow\\ stress\\ \\overline{\\sigma}$', fontsize = 16)\n",
    "    plt.text(minEntries[3] + 0.2*rangeEntries[3], minEntries[3]+ 0.7*rangeEntries[3], '$R = %.8f$' % (R), fontsize=20)\n",
    "    #plt.title(r'$Map\\ of\\ errors\\ \\overline{\\varepsilon}^{p} / T$', fontsize = 20)\n",
    "    if (saveFigures) : plt.savefig(dataPath + '/' + 'NN-' + model.name + '-error-sig.svg', bbox_inches = 'tight', pad_inches = 0)\n",
    "    else: plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-doctor",
   "metadata": {},
   "source": [
    "## Test  efficiency of the NN with random values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-jesus",
   "metadata": {},
   "source": [
    "Scan the content of the testDatabase if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-gallery",
   "metadata": {},
   "outputs": [],
   "source": [
    "testFileName = dataPath + '/Datatest.npz'\n",
    "displayError = False\n",
    "import os.path\n",
    "if (os.path.exists(testFileName)):\n",
    "    \n",
    "    # Load the test Database\n",
    "    testDatabase = np.load(testFileName)\n",
    "    testData = testDatabase['testData']\n",
    "    \n",
    "    # Number of rows of the test database\n",
    "    nrows = testData.shape[0]\n",
    "    \n",
    "    # Extract data from the database\n",
    "    eps_test = testData[:,0].reshape(nrows,1)\n",
    "    epsp_test = testData[:,1].reshape(nrows,1)\n",
    "    T_test = testData[:,2].reshape(nrows,1)\n",
    "    sig_test = testData[:,3].reshape(nrows,1)\n",
    "    \n",
    "    # Computes the log of epsp\n",
    "    epsp_test = np.log(epsp_test/epsLogBase)\n",
    "    \n",
    "    # Creates the input test data with scale\n",
    "    inputTest = (np.hstack([eps_test, epsp_test, T_test]) - np.array(minEntries)[0:3]) / np.array(rangeEntries)[0:3]\n",
    "\n",
    "    # Computes the predicted values and rescale back them\n",
    "    for model in models:\n",
    "        print(\"Model :\", model.name)\n",
    "        sigNN = model.predict(inputTest) * rangeEntries[3] + minEntries[3]\n",
    "    \n",
    "        # Computes the error\n",
    "        error = np.abs(sigNN - sig_test)/sigNN\n",
    "        print(\"Max error is:\", np.max(error))\n",
    "        print(\"Mean error is:\", np.mean(error))\n",
    "        errorSort = np.sort(error, axis=0)\n",
    "        largeErrors = errorSort[errorSort > 5/1000]\n",
    "        print(len(largeErrors),'/',len(error),'points have errors > 0.5 %\\n')\n",
    "\n",
    "        # Display info for large errors\n",
    "        if (displayError):\n",
    "            for err in largeErrors[::-1]:\n",
    "                loc = np.where(error == err)[0][0]\n",
    "                print('err= %5.3f %% for eps= %5.3E, epsp= %5.3E, T= %5.2f' %(error[loc][0]*100, eps_test[loc][0], math.exp(epsp_test[loc][0]), T_test[loc][0]))\n",
    "            print('-----------------------------------------------------------------------\\n')\n",
    "        \n",
    "        plt.figure(figsize = (12, 8))\n",
    "        plt.rc('text', usetex = True)\n",
    "        plt.scatter(eps_test[error >= 0.01], T_test[error >= 0.01], color='black', label=\"$> 1.00 \\%$\")\n",
    "        plt.scatter(eps_test[(error < 0.01) & (error >= 0.005)], T_test[(error < 0.01) & (error >= 0.005)], color='red', label=\"$0.5 - 1.00 \\%$\")\n",
    "        plt.scatter(eps_test[(error < 0.005) & (error >= 0.0025)], T_test[(error < 0.005) & (error >= 0.0025)], color='orange', label=\"$0.25 - 0.50 \\%$\")\n",
    "        plt.scatter(eps_test[(error < 0.0025) & (error >= 0.001)], T_test[(error < 0.0025) & (error >= 0.001)], color='yellow', label=\"$0.10 - 0.25 \\%$\")\n",
    "        plt.scatter(eps_test[error < 0.001], T_test[error < 0.001], color='green', label=\"$< 0.10 \\%$\")\n",
    "        plt.legend(loc = 'lower right',fancybox = True, numpoints = 1, fontsize = 14)\n",
    "        plt.xlabel(r'$Deformation\\ \\overline{\\varepsilon}^{p}$', fontsize = 16)\n",
    "        plt.ylabel(r'$T^{\\circ}C$', fontsize = 16)\n",
    "        plt.title(r'$Map\\ of\\ errors\\ \\overline{\\varepsilon}^{p} / T$', fontsize = 20)\n",
    "        if (saveFigures) : plt.savefig(dataPath + '/' + 'NN-' + model.name + '-error-T-eps.svg', bbox_inches = 'tight', pad_inches = 0)\n",
    "        else: plt.show()\n",
    "        \n",
    "        plt.figure(figsize = (12, 8))\n",
    "        plt.scatter(eps_test[error >= 0.01], np.exp(epsp_test[error >= 0.01]), color='black', label=\"$> 1.00 \\%$\")\n",
    "        plt.scatter(eps_test[(error < 0.01) & (error >= 0.005)], np.exp(epsp_test[(error < 0.01) & (error >= 0.005)]), color='red', label=\"$0.50 - 1.0 \\%$\")\n",
    "        plt.scatter(eps_test[(error < 0.005) & (error >= 0.0025)], np.exp(epsp_test[(error < 0.005) & (error >= 0.0025)]), color='orange', label=\"$0.25 - 0.50 \\%$\")\n",
    "        plt.scatter(eps_test[(error < 0.0025) & (error >= 0.001)], np.exp(epsp_test[(error < 0.0025) & (error >= 0.001)]), color='yellow', label=\"$0.10 - 0.25 \\%$\")\n",
    "        plt.scatter(eps_test[error < 0.001], np.exp(epsp_test[error < 0.001]), color='green', label=\"$< 0.10 \\%$\")\n",
    "        plt.legend(loc = 'lower right',fancybox = True, numpoints = 1, fontsize = 14)\n",
    "        plt.xlabel(r'$Deformation\\ \\overline{\\varepsilon}^{p}$', fontsize = 16)\n",
    "        plt.ylabel(r'$\\dot{\\overline{\\varepsilon}^{p}}$', fontsize = 16)\n",
    "        plt.title(r'$Map\\ of\\ errors\\ \\overline{\\varepsilon}^{p} / \\dot{\\overline{\\varepsilon}^{p}}$', fontsize = 20)\n",
    "        if (saveFigures) : plt.savefig(dataPath + '/' + 'NN-' + model.name + '-error-epsp-eps.svg', bbox_inches = 'tight', pad_inches = 0)\n",
    "        else: plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-houston",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "362.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
