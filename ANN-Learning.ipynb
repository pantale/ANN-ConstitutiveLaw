{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intellectual-romance",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-04T09:59:15.537143Z",
     "start_time": "2021-03-04T09:40:23.562Z"
    }
   },
   "source": [
    "# Identification of a Neural-Network for Constitutive Law"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-excitement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-04T09:59:15.554967Z",
     "start_time": "2021-03-04T09:43:59.056Z"
    }
   },
   "source": [
    "## Preliminary part - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-nomination",
   "metadata": {},
   "source": [
    "To allow inline pictures, run the following block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-roads",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T07:54:47.324316Z",
     "start_time": "2021-03-11T07:54:47.009223Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-jimmy",
   "metadata": {},
   "source": [
    "Import all the useful libraries before first run\n",
    "We need here the classic ones such as:\n",
    "- math\n",
    "- numpy\n",
    "- pandas\n",
    "- matplotlib\n",
    "\n",
    "And for the Neural Network, we also need to import parts of the keras module of TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-batman",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T07:54:49.059562Z",
     "start_time": "2021-03-11T07:54:47.326197Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importation de TensorFlow \n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.metrics import MeanSquaredError\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-bride",
   "metadata": {},
   "source": [
    "**Global definitions**\n",
    "\n",
    "Here after, we define the colors to use for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-helmet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T07:54:49.064311Z",
     "start_time": "2021-03-11T07:54:49.061559Z"
    }
   },
   "outputs": [],
   "source": [
    "saveFigures = True\n",
    "colorlist = ['#bb0000', '#00bb00', \"#0000bb\", '#bbbb00', '#bb00bb', \"#00bbbb\", '#bbbbbb', '#770000', '#007700', \"#000077\", '#777700', '#770077', \"#007777\", '#777777', '#440000', '#004400', \"#000044\", '#444400', '#440044', \"#0044444\", '#444444','#000000']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-fishing",
   "metadata": {},
   "source": [
    "Useful functions to define the number of subplots to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-recruitment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T07:54:49.072414Z",
     "start_time": "2021-03-11T07:54:49.066132Z"
    }
   },
   "outputs": [],
   "source": [
    "baseSize = (8,6) # Base size of a subplot\n",
    "\n",
    "def sbPlot(n):\n",
    "    if (n == 1): return 1, 1\n",
    "    if (n <= 2): return 1, 2\n",
    "    if (n <= 4): return 2, 2\n",
    "    if (n <= 6): return 3, 2\n",
    "    if (n <= 9): return 3, 3\n",
    "    if (n <=12): return 4, 3\n",
    "    return 0,0\n",
    "\n",
    "def sbPlotSize(n):\n",
    "    x, y = sbPlot(n)\n",
    "    return baseSize[0]*y, baseSize[1]*x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-jefferson",
   "metadata": {},
   "source": [
    "Used to plot an Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-logic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T07:54:49.083614Z",
     "start_time": "2021-03-11T07:54:49.074325Z"
    }
   },
   "outputs": [],
   "source": [
    "def getDigitsFrom(n):\n",
    "    d = int(math.log10(abs(n)))\n",
    "    return n*10**(-d + 1), d - 1\n",
    "\n",
    "def plotHisto(Y, labels, bottom, top, xlab, ylab, figname, title):\n",
    "    numb = len(Y)\n",
    "    X = np.arange(numb) +1\n",
    "\n",
    "    plt.figure(figsize = (12, 9)) # for a4 landscape\n",
    "    plt.rc('text', usetex=True)\n",
    "    plt.rcParams['xtick.labelsize'] = 16\n",
    "    plt.rcParams['ytick.labelsize'] = 16\n",
    "\n",
    "    plt.bar(X, Y, color = '#770000')\n",
    "\n",
    "    for x, y in zip(X, Y):\n",
    "        u,v = getDigitsFrom(y)\n",
    "        plt.text(x, y + 0.01*(top-bottom), '$%.2f \\\\times 10^{%d}$' % (u,v), ha='center', va='bottom', fontsize=16)\n",
    "\n",
    "    for i in range(0,numb):\n",
    "        if (Y[i]> 0.5*top):\n",
    "            plt.text(X[i], (top-bottom)/10 + bottom , labels[i], ha='center', va='bottom', fontsize=20, rotation=90, color='white')\n",
    "        else:\n",
    "            plt.text(X[i], 5*(top-bottom)/10 + bottom , labels[i], ha='center', va='bottom', fontsize=20, rotation=90, color='black')\n",
    "\n",
    "    plt.ylim(bottom, top)\n",
    "\n",
    "    plt.ylabel(ylab, fontsize = 20)\n",
    "    plt.xlabel(xlab, fontsize = 20)\n",
    "    plt.title(title, fontsize = 20)\n",
    "\n",
    "    plt.tick_params(axis='x',top=False, bottom=False, left=False, right=False,\n",
    "                labelleft=False, labelbottom=False)\n",
    "    plt.grid(True)\n",
    "    if (saveFigures) : plt.savefig(figname)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-supervision",
   "metadata": {},
   "source": [
    "## Definition of the data file to use \n",
    "We define here after the directory and the Excel Datafile used for the Neural-Network construction."
   ]
  },
  {
   "cell_type": "raw",
   "id": "absent-constant",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-04T14:21:57.518068Z",
     "start_time": "2021-03-04T14:21:57.512879Z"
    }
   },
   "source": [
    "dataPath = 'ANN-Zhou-Law'\n",
    "dataFile = 'ExperimentsTreated.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-learning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T07:54:49.087589Z",
     "start_time": "2021-03-11T07:54:49.085360Z"
    }
   },
   "outputs": [],
   "source": [
    "dataPath = 'ANN-JohnsonCook'\n",
    "dataFile = 'JC-Experiments.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-closure",
   "metadata": {},
   "source": [
    "## Open the Excel file and import data\n",
    "Read the content of the Excel file using the pandas library method read_excel.\n",
    "\n",
    "In this datafile, temperatures must be on separate sheets, while the corresponding sheets contains stress / strain / strain-rate values: $$\\overline{\\sigma}=f({\\overline{\\varepsilon}^p},\\dot{\\overline{\\varepsilon}^p})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-finance",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T07:54:49.427148Z",
     "start_time": "2021-03-11T07:54:49.089146Z"
    }
   },
   "outputs": [],
   "source": [
    "excelData = pd.read_excel(dataPath + '/' + dataFile, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-accessory",
   "metadata": {},
   "source": [
    "Extract the list of temperatures $T$ from the the Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-blake",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T07:54:49.436622Z",
     "start_time": "2021-03-11T07:54:49.429807Z"
    }
   },
   "outputs": [],
   "source": [
    "len(excelData)\n",
    "temperatures = []\n",
    "temps = list(excelData.keys())\n",
    "for T in temps:\n",
    "    temperatures.append(int(T.replace('째C','')))\n",
    "temperaturesOrig = temperatures\n",
    "nTemp = len(temperatures)\n",
    "nTemp, temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-tunnel",
   "metadata": {},
   "source": [
    "Extract the list of plastic strain rates $\\dot{\\overline{\\varepsilon}^p}$ from the the Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-blond",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T07:54:49.446941Z",
     "start_time": "2021-03-11T07:54:49.438919Z"
    }
   },
   "outputs": [],
   "source": [
    "dataSheet = excelData[str(temperatures[0]) + '째C']\n",
    "epsp = []\n",
    "for i in range(1, dataSheet.shape[1]):\n",
    "    epsp.append(dataSheet.columns[i])\n",
    "epspOrig = epsp\n",
    "nEpsp = len(epsp)\n",
    "nEpsp, epsp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-child",
   "metadata": {},
   "source": [
    "Extract all data from the panda sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-begin",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T07:54:49.452666Z",
     "start_time": "2021-03-11T07:54:49.448403Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read first sheet\n",
    "dataSheet = excelData[str(temperatures[0]) + '째C']\n",
    "data = dataSheet.values\n",
    "eps = data[:,0]\n",
    "sig = data[:,1:]\n",
    "# Append the other sheets\n",
    "for T in temperatures[1:]:\n",
    "    dataSheet = excelData[str(T) + '째C']\n",
    "    data = dataSheet.values\n",
    "    sig = np.append(sig, data[:,1:], axis=1)\n",
    "nEps = len(eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-production",
   "metadata": {},
   "source": [
    "Creates two arrays for temperatures $T$ and plastic strain rates $\\dot{\\overline{\\varepsilon}^p}$ and transforms the values of the plastic strain rates $\\dot{\\overline{\\varepsilon}^p}$ into its logarithmic part.\n",
    "\n",
    "**It is very important** to transform the epsp data into its logarithmic part since entries of the ANN show enhance a linear behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-course",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T07:54:49.462436Z",
     "start_time": "2021-03-11T07:54:49.454405Z"
    }
   },
   "outputs": [],
   "source": [
    "temperatures = np.array(temperatures)\n",
    "epsp = np.array(epsp)\n",
    "epsLogBase = epsp.min()\n",
    "epsp = np.log(epsp/epsLogBase)\n",
    "epspArray = np.tile(epsp,temperatures.shape[0])\n",
    "temperaturesArray = temperatures.repeat(epsp.shape[0])\n",
    "epspArray.shape, epspArray, temperaturesArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-danish",
   "metadata": {},
   "source": [
    "Transform all data into an array of 4 columns where:\n",
    "- $\\overline{\\varepsilon}^p$ : is in first column\n",
    "- $\\dot{\\overline{\\varepsilon}^p}$ : is in second column\n",
    "- $T$ : is in third column\n",
    "- $\\overline{\\sigma}$ : is th fourth column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-lightning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T07:54:49.476651Z",
     "start_time": "2021-03-11T07:54:49.463835Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entries = np.empty([epspArray.shape[0]*eps.shape[0], 4])\n",
    "row = 0\n",
    "colEps = 0\n",
    "colEpsp = 1\n",
    "colT = 2\n",
    "colSig = 3\n",
    "for i in range(epspArray.shape[0]):\n",
    "    for j in range(eps.shape[0]):\n",
    "        entries[row, colEps] = eps[j]\n",
    "        entries[row, colEpsp] = epspArray[i]\n",
    "        entries[row, colT] = temperaturesArray[i]\n",
    "        entries[row, colSig] = sig[j,i]\n",
    "        row += 1\n",
    "entries.shape, entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-badge",
   "metadata": {},
   "source": [
    "Then, we must normalize all the entries, so that they vary within the range $[0:1]$.\n",
    "\n",
    "- We compute the minEntries and rangeEntries variable that contains all the min and max Values\n",
    "- We normalize the data by dividing all variables using: $$X_n = \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n",
    "\n",
    "To rebuid data, we must use: $$ X = X_n(X_{max} - X_{min}) + X_{min}$$"
   ]
  },
  {
   "cell_type": "raw",
   "id": "casual-orange",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T07:55:00.478359Z",
     "start_time": "2021-03-11T07:55:00.460182Z"
    }
   },
   "source": [
    "minEntries = entries.min(axis=0)\n",
    "NNentries = entries - np.array(minEntries)\n",
    "rangeEntries = NNentries.max(axis=0)\n",
    "NNentries = NNentries / np.array(rangeEntries)\n",
    "minEntries, rangeEntries, NNentries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-pharmaceutical",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T08:41:57.598912Z",
     "start_time": "2021-03-11T08:41:57.592826Z"
    }
   },
   "outputs": [],
   "source": [
    "minEntries = entries.min(axis=0)\n",
    "maxEntries = entries.max(axis=0)\n",
    "rangeEntries = maxEntries - minEntries\n",
    "NNentries = (entries - minEntries) / rangeEntries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-ending",
   "metadata": {},
   "source": [
    "To check the previous process precision, and lost of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-programmer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T08:18:13.654167Z",
     "start_time": "2021-03-11T08:18:13.643540Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Max error for normalized data is :\",(NNentries * rangeEntries + minEntries - entries).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-still",
   "metadata": {},
   "source": [
    "## Plot the original data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-winner",
   "metadata": {},
   "source": [
    "Plots the original data where subplots are dependents on the temperature $T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-deployment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T08:18:19.040808Z",
     "start_time": "2021-03-11T08:18:16.434839Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = sbPlotSize(nTemp))\n",
    "plt.rc('text', usetex = True)\n",
    "plt.subplots_adjust(hspace = 0.3)\n",
    "idx = 1\n",
    "for T in temperatures:\n",
    "    xs, ys = sbPlot(nTemp)\n",
    "    plt.subplot(xs, ys, idx)\n",
    "    cl = 0\n",
    "    for epspv, epspvv in zip(epsp, epspOrig):\n",
    "        # filter on T\n",
    "        data = NNentries[(entries[:,colT]==T) & (entries[:,colEpsp]==epspv), :]\n",
    "        # Plot the curves\n",
    "        plt.plot(data[:,colEps]*rangeEntries[colEps]+minEntries[colEps], data[:,colSig]*rangeEntries[colSig]+minEntries[colEps], color=colorlist[cl], label=r'$\\dot{\\overline{\\varepsilon}^p}=' + str(epspvv) + '\\ s^{-1}$', marker='o', linewidth = 3)\n",
    "        cl += 1\n",
    "    plt.legend(loc = 'lower right',fancybox = True, numpoints = 1, fontsize = 10)\n",
    "    plt.grid()\n",
    "    plt.xlabel(r'$Deformation\\ \\overline{\\varepsilon}^{p}$', fontsize = 16)\n",
    "    plt.ylabel(r'$Stress\\ \\overline{\\sigma}$', fontsize = 16)\n",
    "    plt.title(r'$T=' + str(T) + '^{\\circ}C$', fontsize = 16)\n",
    "    idx += 1\n",
    "if (saveFigures) : plt.savefig(dataPath + '/' + 'OriginalData-T.svg', bbox_inches = 'tight', pad_inches = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-celebration",
   "metadata": {},
   "source": [
    "Plots the original data where subplots are dependents on the plastic strain rate $\\dot{\\overline{\\varepsilon}^p}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-mainstream",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T08:18:26.922295Z",
     "start_time": "2021-03-11T08:18:24.844002Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = sbPlotSize(nEpsp))\n",
    "plt.rc('text', usetex = True)\n",
    "plt.subplots_adjust(hspace = 0.3)\n",
    "idx = 1\n",
    "for epspv, epspvv in zip(epsp, epspOrig):\n",
    "    xs, ys = sbPlot(nEpsp)\n",
    "    plt.subplot(xs, ys, idx)\n",
    "    cl = 0\n",
    "    for T, TOrig in zip(temperatures, temperaturesOrig):\n",
    "        # filter on T\n",
    "        data = NNentries[(entries[:,colT]==T) & (entries[:,colEpsp]==epspv), :]\n",
    "        # Plot the curves\n",
    "        plt.plot(data[:,colEps]*rangeEntries[colEps]+minEntries[colEps], data[:,colSig]*rangeEntries[colSig]+minEntries[colSig], color=colorlist[cl], label=r'$T=' + str(TOrig) + '^{\\circ}C$', marker = 'o', linewidth = 3)\n",
    "        cl += 1\n",
    "    plt.legend(loc = 'lower right',fancybox = True, numpoints = 1, fontsize = 10)\n",
    "    plt.grid()\n",
    "    plt.xlabel(r'$Deformation\\ \\overline{\\varepsilon}^{p}$', fontsize = 16)\n",
    "    plt.ylabel(r'$Stress\\ \\overline{\\sigma}$', fontsize = 16)\n",
    "    plt.title(r'$\\dot{\\overline{\\varepsilon}^p}=' + str(epspvv) + '\\ s^{-1}$', fontsize = 16)\n",
    "    idx += 1\n",
    "if (saveFigures) : plt.savefig(dataPath + '/' + 'OriginalData-epsp.svg', bbox_inches = 'tight', pad_inches = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-council",
   "metadata": {},
   "source": [
    "## Data form transformation for the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-thesis",
   "metadata": {},
   "source": [
    "Separate input and output from data.\n",
    "\n",
    "Input contains:\n",
    "- $\\overline{\\varepsilon}^p$ : is in first column\n",
    "- $\\dot{\\overline{\\varepsilon}^p}$ : is in second column\n",
    "- $T$ : is in third column\n",
    "\n",
    "Output contains:\n",
    "- $\\overline{\\sigma}$ : the only column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-edwards",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T08:19:18.399261Z",
     "start_time": "2021-03-11T08:19:18.385884Z"
    }
   },
   "outputs": [],
   "source": [
    "NNinput = NNentries[:,colEps:colSig]\n",
    "NNoutput = NNentries[:,colSig]\n",
    "NNinput.shape, NNoutput.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-poison",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T13:40:24.550645Z",
     "start_time": "2021-03-02T13:40:24.538720Z"
    }
   },
   "source": [
    "## Neural Network set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-ranch",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T08:19:29.326910Z",
     "start_time": "2021-03-11T08:19:29.162442Z"
    }
   },
   "outputs": [],
   "source": [
    "models = []              # The list of models to use\n",
    "convergenceCurves = []   # To store convergence curves\n",
    "\n",
    "#FACT = ['relu', 'sigmoid', 'tanh']\n",
    "#COUCH = [3, 5, 7, 9]\n",
    "FACT = ['relu', 'sigmoid', 'tanh']\n",
    "COUCH = [5, 7, 9]\n",
    "#COUCH = [[5,2], [5,3], [5,4]]\n",
    "\n",
    "FACT = ['tanh']\n",
    "COUCH = [5, 7]\n",
    "\n",
    "for f in FACT:\n",
    "    for c in COUCH:\n",
    "        desc = '3'\n",
    "        model = Sequential()\n",
    "        if type(c) == list:\n",
    "            fst = True\n",
    "            for k in c:\n",
    "                if (fst): model.add(Dense(k, input_dim = 3, activation = f))\n",
    "                else: model.add(Dense(k, activation = f))\n",
    "                fst = False\n",
    "                desc += '-' + str(k)\n",
    "        else:\n",
    "            model.add(Dense(c, input_dim = 3, activation = f))\n",
    "            desc += '-' + str(c)\n",
    "        model.add(Dense(1))\n",
    "        desc += '-1-'+f\n",
    "        model._name = desc\n",
    "        models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-crystal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-04T10:41:55.238673Z",
     "start_time": "2021-03-04T10:41:54.838684Z"
    }
   },
   "source": [
    "Initialize models and apply solver and optimizer for each one.\n",
    "\n",
    "Here, we choose the **adam** solver and a **mse** error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-girlfriend",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T08:19:33.864625Z",
     "start_time": "2021-03-11T08:19:33.795233Z"
    }
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-adaptation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-04T10:41:55.395023Z",
     "start_time": "2021-03-04T10:41:55.364828Z"
    }
   },
   "source": [
    "## Solve the parameters of all ANN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-struggle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T08:41:57.206358Z",
     "start_time": "2021-03-11T08:20:06.085777Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iterationsNumber = 20 # Define the number of iteration to do\n",
    "epochNumber = 1000    # Define the number of epoch for each iteration\n",
    "\n",
    "for model in models:\n",
    "    hist = np.array([])\n",
    "    print(\"MODEL :\", model.name)\n",
    "    for i in range(iterationsNumber):\n",
    "        history = model.fit(NNinput, NNoutput, epochs = epochNumber, verbose = 0)\n",
    "        loss = history.history['loss']\n",
    "        hist = np.append(hist, loss)\n",
    "        print(\"Iteration :\", i + 1, \"/\", iterationsNumber, '-> %8.6E' % loss[-1], \"  \", end = '\\r')\n",
    "    print(\"\\n\")\n",
    "    convergenceCurves.append(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-dryer",
   "metadata": {},
   "source": [
    "## Plot results of the optimization procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-cloud",
   "metadata": {},
   "source": [
    "First, we plot the convergence curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-retro",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T09:03:54.709156Z",
     "start_time": "2021-03-11T09:03:54.372497Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 9))\n",
    "plt.rc('text', usetex = True)\n",
    "cl = 0\n",
    "for model, curve in zip(models, convergenceCurves):\n",
    "    shortCurve = curve[::10]\n",
    "    x = np.linspace(0, len(curve), len(shortCurve))\n",
    "    plt.plot(x, shortCurve, label = model.name, color=colorlist[cl], linewidth = 3)\n",
    "    cl += 1\n",
    "plt.grid()\n",
    "plt.xlabel(r'$Iteration$', fontsize = 16)\n",
    "plt.ylim(0, 5e-3)\n",
    "plt.ylabel(r'$Quadratic\\ Error\\ \\Delta E$', fontsize = 16)\n",
    "plt.title(r'$Global\\ convergence\\ of\\ the\\ Neural\\ Network$', fontsize = 16)\n",
    "plt.legend(loc = 'upper right',fancybox = True, numpoints = 1, fontsize = 14)\n",
    "if (saveFigures) : plt.savefig(dataPath + '/' + 'Convergence.svg', bbox_inches = 'tight', pad_inches = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-suspect",
   "metadata": {},
   "source": [
    "The we plot the history of all models precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-necklace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T09:04:06.376068Z",
     "start_time": "2021-03-11T09:04:04.726485Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 9))\n",
    "plt.rc('text', usetex = True)\n",
    "convs = np.array([])\n",
    "labels = []\n",
    "for model, curve in zip(models, convergenceCurves):\n",
    "    lastzone = int(len(curve)/20) # 5% des points pris en compte pour la convergence globale\n",
    "    convs = np.append(convs, curve[-lastzone:].mean())\n",
    "    labels.append(model.name)\n",
    "bottom = 0\n",
    "top = convs.max()*1.1\n",
    "ylab = r'$Quadratic\\ Error\\ \\Delta E$'\n",
    "xlab = r'$Model\\ used$'\n",
    "figname = dataPath + '/' + 'Precision.svg'\n",
    "title = r'$Global\\ precision\\ of\\ the\\ Neural\\ Network$'\n",
    "plotHisto(convs, labels, bottom, top, xlab, ylab, figname, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-highlight",
   "metadata": {},
   "source": [
    "Plots the results of the model where all subplots are dependents on the plastic strain rate $\\dot{\\overline{\\varepsilon}^p}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-racing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T09:04:14.036527Z",
     "start_time": "2021-03-11T09:04:10.071708Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    predictedOutput = model.predict(NNinput)\n",
    "    plotPredicted = predictedOutput.reshape(nEpsp*nTemp, nEps).T\n",
    "    pcol = 0\n",
    "    plt.figure(figsize = sbPlotSize(nTemp))\n",
    "    plt.rc('text', usetex = True)\n",
    "    plt.subplots_adjust(hspace = 0.3)\n",
    "    idx = 1\n",
    "    for T, TOrig in zip(temperatures, temperaturesOrig):\n",
    "        pli = 0\n",
    "        xs, ys = sbPlot(nTemp)\n",
    "        plt.subplot(xs, ys, idx)\n",
    "        for epspv, epspvAff in zip(epsp, epspOrig):\n",
    "            # filter on T\n",
    "            data = NNentries[(entries[:,colT]==T) & (entries[:,colEpsp]==epspv), :]\n",
    "            # Plot the curves\n",
    "            plt.plot(data[:,colEps]*rangeEntries[colEps]+minEntries[colEps], data[:,colSig]*rangeEntries[colSig]+minEntries[colSig], color=colorlist[pli], label=r'$\\dot{\\overline{\\varepsilon}^p}=' + str(epspvAff) + '\\ s^{-1}$', marker = 'o', linestyle='none')\n",
    "            plt.plot(data[:,colEps]*rangeEntries[colEps]+minEntries[colEps], plotPredicted[:,pcol*nEpsp+pli]*rangeEntries[colSig]+minEntries[colSig], colorlist[pli], linewidth = 3)\n",
    "            pli += 1\n",
    "        pcol += 1\n",
    "        plt.legend(loc = 'lower right',fancybox = True, numpoints = 1, fontsize = 14)\n",
    "        plt.grid()\n",
    "        plt.xlabel(r'$Deformation\\ \\overline{\\varepsilon}^{p}$', fontsize = 16)\n",
    "        plt.ylabel(r'$Stress\\ \\overline{\\sigma}$', fontsize = 16)\n",
    "        plt.title(r'$T=' + str(TOrig) + '^{\\circ}C$', fontsize = 16)\n",
    "        idx += 1\n",
    "    if (saveFigures) : plt.savefig(dataPath + '/' + 'NN-' + model.name + '-epsp.svg', bbox_inches = 'tight', pad_inches = 0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-interest",
   "metadata": {},
   "source": [
    "Plots the results of the model where all subplots are dependents on the temperature $T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-vietnamese",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T09:04:25.273515Z",
     "start_time": "2021-03-11T09:04:22.358871Z"
    }
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    predictedOutput = model.predict(NNinput)\n",
    "    plotPredicted = predictedOutput.reshape(nEpsp*nTemp, nEps).T\n",
    "    pcol = 0\n",
    "    plt.figure(figsize = sbPlotSize(nEpsp))\n",
    "    plt.rc('text', usetex = True)\n",
    "    plt.subplots_adjust(hspace = 0.3)\n",
    "    idx = 1\n",
    "    for epspv, epspvAff in zip(epsp, epspOrig):\n",
    "        pli = 0\n",
    "        xs, ys = sbPlot(nEpsp)\n",
    "        plt.subplot(xs, ys, idx)\n",
    "        for T, TOrig in zip(temperatures, temperaturesOrig):\n",
    "            # filter on T\n",
    "            data = NNentries[(entries[:,colT]==T) & (entries[:,colEpsp]==epspv), :]\n",
    "            # Plot the curves\n",
    "            plt.plot(data[:,colEps]*rangeEntries[colEps]+minEntries[colEps], data[:,colSig]*rangeEntries[colSig]+minEntries[colSig], color=colorlist[pli], label=r'$T=' + str(TOrig) + '^{\\circ}C$', marker = 'o', linestyle='none')\n",
    "            plt.plot(data[:,colEps]*rangeEntries[colEps]+minEntries[colEps], plotPredicted[:,pcol+nEpsp*pli]*rangeEntries[colSig]+minEntries[colSig], colorlist[pli], linewidth = 3)\n",
    "            pli += 1\n",
    "        pcol += 1\n",
    "        plt.legend(loc = 'lower right',fancybox = True, numpoints = 1, fontsize = 14)\n",
    "        plt.grid()\n",
    "        plt.xlabel(r'$Deformation\\ \\overline{\\varepsilon}^{p}$', fontsize = 16)\n",
    "        plt.ylabel(r'$Stress\\ \\overline{\\sigma}$', fontsize = 16)\n",
    "        plt.title(r'$\\dot{\\overline{\\varepsilon}^p}=' + str(epspvAff) + '\\ s^{-1}$', fontsize = 16)\n",
    "        idx += 1\n",
    "    if (saveFigures) : plt.savefig(dataPath + '/' + 'NN-' + model.name + '-T.svg', bbox_inches = 'tight', pad_inches = 0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-inside",
   "metadata": {},
   "source": [
    "## Save results for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-arabic",
   "metadata": {},
   "source": [
    "Save the Tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-garden",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T09:04:30.147730Z",
     "start_time": "2021-03-11T09:04:29.060429Z"
    }
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    model.save(dataPath + '/' + model.name)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "photographic-greensboro",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T14:05:52.496711Z",
     "start_time": "2021-03-05T14:05:52.092542Z"
    }
   },
   "source": [
    "models = []              # The list of models to use\n",
    "modNames = ['3-5-1-tanh', '3-7-1-tanh']\n",
    "for modName in modNames:\n",
    "    new_model = tf.keras.models.load_model(dataPath + '/' + modName)\n",
    "    models.append(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-correction",
   "metadata": {},
   "source": [
    "Save all internal arrays for direct use of the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-folder",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T09:04:33.825849Z",
     "start_time": "2021-03-11T09:04:33.764246Z"
    }
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    if len(model.layers)==1: \n",
    "        np.savez(dataPath + '/' + model.name,\n",
    "                 logBase = np.array([epsLogBase]),\n",
    "                 minEntries=minEntries, \n",
    "                 maxEntries=maxEntries,\n",
    "                 w1=model.layers[0].get_weights()[0].T, \n",
    "                 b1=model.layers[0].get_weights()[1].reshape(len(model.layers[0].get_weights()[1]),1))\n",
    "    if len(model.layers)==2: \n",
    "        np.savez(dataPath + '/' + model.name, \n",
    "                 logBase = np.array([epsLogBase]),\n",
    "                 minEntries=minEntries, \n",
    "                 maxEntries=maxEntries,\n",
    "                 w1=model.layers[0].get_weights()[0].T, \n",
    "                 b1=model.layers[0].get_weights()[1].reshape(len(model.layers[0].get_weights()[1]),1),\n",
    "                 w2=model.layers[1].get_weights()[0].T, \n",
    "                 b2=model.layers[1].get_weights()[1].reshape(len(model.layers[1].get_weights()[1]),1))\n",
    "    if len(model.layers)==3: \n",
    "        np.savez(dataPath + '/' + model.name, \n",
    "                 logBase = np.array([epsLogBase]),\n",
    "                 minEntries=minEntries, \n",
    "                 maxEntries=maxEntries,\n",
    "                 w1=model.layers[0].get_weights()[0].T, \n",
    "                 b1=model.layers[0].get_weights()[1].reshape(len(model.layers[0].get_weights()[1]),1),\n",
    "                 w2=model.layers[1].get_weights()[0].T, \n",
    "                 b2=model.layers[1].get_weights()[1].reshape(len(model.layers[1].get_weights()[1]),1),\n",
    "                 w3=model.layers[2].get_weights()[0].T, \n",
    "                 b3=model.layers[2].get_weights()[1].reshape(len(model.layers[2].get_weights()[1]),1)) \n",
    "    if len(model.layers)==4: \n",
    "        np.savez(dataPath + '/' + model.name, \n",
    "                 logBase = np.array([epsLogBase]),\n",
    "                 minEntries=minEntries, \n",
    "                 maxEntries=maxEntries,\n",
    "                 w1=model.layers[0].get_weights()[0].T, \n",
    "                 b1=model.layers[0].get_weights()[1].reshape(len(model.layers[0].get_weights()[1]),1),\n",
    "                 w2=model.layers[1].get_weights()[0].T, \n",
    "                 b2=model.layers[1].get_weights()[1].reshape(len(model.layers[1].get_weights()[1]),1),\n",
    "                 w3=model.layers[2].get_weights()[0].T, \n",
    "                 b3=model.layers[2].get_weights()[1].reshape(len(model.layers[2].get_weights()[1]),1),\n",
    "                 w4=model.layers[3].get_weights()[0].T, \n",
    "                 b4=model.layers[3].get_weights()[1].reshape(len(model.layers[3].get_weights()[1]),1))             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-doctor",
   "metadata": {},
   "source": [
    "## Test  efficiency of the NN with random values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-jesus",
   "metadata": {},
   "source": [
    "Scan the content of the testDatabase if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-gallery",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T09:08:58.138626Z",
     "start_time": "2021-03-11T09:08:56.396219Z"
    }
   },
   "outputs": [],
   "source": [
    "testFileName = dataPath + '/Datatest.npz'\n",
    "import os.path\n",
    "if (os.path.exists(testFileName)):\n",
    "    \n",
    "    # Load the test Database\n",
    "    testDatabase = np.load(testFileName)\n",
    "    testData = testDatabase['testData']\n",
    "    \n",
    "    # Number of rows of the test database\n",
    "    nrows = testData.shape[0]\n",
    "    \n",
    "    # Extract data from the database\n",
    "    eps_test = testData[:,0].reshape(nrows,1)\n",
    "    epsp_test = testData[:,1].reshape(nrows,1)\n",
    "    T_test = testData[:,2].reshape(nrows,1)\n",
    "    sig_test = testData[:,3].reshape(nrows,1)\n",
    "    \n",
    "    # Computes the log of epsp\n",
    "    epsp_test = np.log(epsp_test/epsLogBase)\n",
    "    \n",
    "    # Creates the input test data with scale\n",
    "    inputTest = (np.hstack([eps_test, epsp_test, T_test]) - np.array(minEntries)[0:3]) / np.array(rangeEntries)[0:3]\n",
    "    \n",
    "    # Computes the predicted values and rescale back them\n",
    "    for model in models:\n",
    "        print(\"Model :\", model.name)\n",
    "        sigNN = model.predict(inputTest) * rangeEntries[3] + minEntries[3]\n",
    "    \n",
    "        # Computes the error\n",
    "        error = np.abs(sigNN - sig_test)/sigNN\n",
    "        print(\"Max error is:\", np.max(error))\n",
    "        print(\"Mean error is:\", np.mean(error))\n",
    "        errorSort = np.sort(error, axis=0)\n",
    "        largeErrors = errorSort[errorSort > 5/1000]\n",
    "        print(len(largeErrors),'/',len(error),'points have errors > 0.5 %\\n')\n",
    "\n",
    "        # Display info for large errors\n",
    "        for err in largeErrors[::-1]:\n",
    "            loc = np.where(error == err)[0][0]\n",
    "            print('err= %5.3f %% for eps= %5.3E, epsp= %5.3E, T= %5.2f' %(error[loc][0]*100, eps_test[loc][0], math.exp(epsp_test[loc][0]), T_test[loc][0]))\n",
    "        print('-----------------------------------------------------------------------\\n')\n",
    "        \n",
    "        plt.figure(figsize = (12, 8))\n",
    "        plt.rc('text', usetex = True)\n",
    "        plt.scatter(eps_test[error >= 0.01], T_test[error >= 0.01], color='black', label=\"$> 1.00 \\%$\")\n",
    "        plt.scatter(eps_test[(error < 0.01) & (error >= 0.005)], T_test[(error < 0.01) & (error >= 0.005)], color='red', label=\"$0.5 - 1.00 \\%$\")\n",
    "        plt.scatter(eps_test[(error < 0.005) & (error >= 0.0025)], T_test[(error < 0.005) & (error >= 0.0025)], color='orange', label=\"$0.25 - 0.50 \\%$\")\n",
    "        plt.scatter(eps_test[(error < 0.0025) & (error >= 0.001)], T_test[(error < 0.0025) & (error >= 0.001)], color='yellow', label=\"$0.10 - 0.25 \\%$\")\n",
    "        plt.scatter(eps_test[error < 0.001], T_test[error < 0.001], color='green', label=\"$< 0.10 \\%$\")\n",
    "        plt.legend(loc = 'lower right',fancybox = True, numpoints = 1, fontsize = 14)\n",
    "        plt.xlabel(r'$Deformation\\ \\overline{\\varepsilon}^{p}$', fontsize = 16)\n",
    "        plt.ylabel(r'$T^{\\circ}C$', fontsize = 16)\n",
    "        plt.title(r'$Map\\ of\\ errors\\ \\overline{\\varepsilon}^{p} / T$', fontsize = 20)\n",
    "        if (saveFigures) : plt.savefig(dataPath + '/' + 'NN-' + model.name + '-error-T-eps.svg', bbox_inches = 'tight', pad_inches = 0)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize = (12, 8))\n",
    "        plt.scatter(eps_test[error >= 0.01], np.exp(epsp_test[error >= 0.01]), color='black', label=\"$> 1.00 \\%$\")\n",
    "        plt.scatter(eps_test[(error < 0.01) & (error >= 0.005)], np.exp(epsp_test[(error < 0.01) & (error >= 0.005)]), color='red', label=\"$0.50 - 1.0 \\%$\")\n",
    "        plt.scatter(eps_test[(error < 0.005) & (error >= 0.0025)], np.exp(epsp_test[(error < 0.005) & (error >= 0.0025)]), color='orange', label=\"$0.25 - 0.50 \\%$\")\n",
    "        plt.scatter(eps_test[(error < 0.0025) & (error >= 0.001)], np.exp(epsp_test[(error < 0.0025) & (error >= 0.001)]), color='yellow', label=\"$0.10 - 0.25 \\%$\")\n",
    "        plt.scatter(eps_test[error < 0.001], np.exp(epsp_test[error < 0.001]), color='green', label=\"$< 0.10 \\%$\")\n",
    "        plt.legend(loc = 'lower right',fancybox = True, numpoints = 1, fontsize = 14)\n",
    "        plt.xlabel(r'$Deformation\\ \\overline{\\varepsilon}^{p}$', fontsize = 16)\n",
    "        plt.ylabel(r'$\\dot{\\overline{\\varepsilon}^{p}}$', fontsize = 16)\n",
    "        plt.title(r'$Map\\ of\\ errors\\ \\overline{\\varepsilon}^{p} / \\dot{\\overline{\\varepsilon}^{p}}$', fontsize = 20)\n",
    "        if (saveFigures) : plt.savefig(dataPath + '/' + 'NN-' + model.name + '-error-epsp-eps.svg', bbox_inches = 'tight', pad_inches = 0)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-signal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "362.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
